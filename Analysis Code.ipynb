{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810dce4a-fc14-471c-b749-df5eba810d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mne.decoding import CSP\n",
    "from sklearn.svm import SVC\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "import pylab\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea424a36-c66b-4e5c-839f-415bfbb643cc",
   "metadata": {},
   "source": [
    "**Table 2**\n",
    "\n",
    "**Model trained** on Clean trials from tradtional RSVP\n",
    "\n",
    "**Model tested** on clean, bad and combination of clean & bad trials from each tradtional as well as Intentionally Contaminated IC-RSVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ea873-fd26-4ed8-8d81-3fcf1259a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "THR=100e-6 # P2P threshold for trial rejection \n",
    "# Open a CSV file for writing\n",
    "with open('RocAuc_TrainClean_100uv.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['P_Num', 'RSVP-All', 'RSVP-Clean','RSVP-Rejected', 'XXXXXXXXXXX', 'Body-All', 'Body-Clean','Body-Rejected', 'XXXXXXXXXXX',\n",
    "                     \n",
    "                     'Talk-All', 'Talk-Clean','Talk-Rejected', 'XXXXXXXXXXX', 'Head-All', 'Head-Clean','Head-Rejected'])\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        P_Num = i\n",
    "        \n",
    "        \n",
    "        \n",
    "        #P_Num=10\n",
    "\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "        def process_file(edf_file_path, csv_file_path, filename):\n",
    "            raw=mne.io.read_raw_edf(edf_file_path, preload=True)\n",
    "\n",
    "            # Pick 6 out of 32 channels\n",
    "            #channel_names_to_select = ['Fz-CPz','Cz-CPz', 'P3-CPz', 'Pz-CPz', 'P4-CPz','Oz-CPz']\n",
    "            #raw.pick_channels(channel_names_to_select)\n",
    "\n",
    "            #Applying lowpass filter at 30Hz\n",
    "            raw = raw.filter(.1, 30, method='iir')\n",
    "\n",
    "            # Resampling data from 1000sps to 100sps\n",
    "            raw = raw.resample(sfreq=100)\n",
    "\n",
    "\n",
    "            #Re-Referencing the data to common average reference (CAR)\n",
    "            raw= raw.set_eeg_reference(ref_channels='average')\n",
    "\n",
    "\n",
    "            data = raw.get_data()\n",
    "            Samples=len(data[0])\n",
    "\n",
    "            annt=mne.events_from_annotations(raw, event_id='auto', regexp='^(?![Bb][Aa][Dd]|[Ee][Dd][Gg][Ee]).*$', use_rounding=True, chunk_duration=None, verbose=None)\n",
    "            ann0=annt[0]\n",
    "            Markers_Raw=ann0[:,0]\n",
    "            L_annt_r=len(Markers_Raw)\n",
    "\n",
    "            if (L_annt_r == 362):\n",
    "                Markers=Markers_Raw[1:-1] # :- Extras = 1 start and 1 end\n",
    "\n",
    "\n",
    "            # Subject 5 -- Session 4 -- X1 :-\n",
    "            ## This process is described and done in detail in another notebook.\n",
    "            # there are total 371 markers instead of 362.\n",
    "            # 1 marker from start and 9 markers from end are extra markers.\n",
    "            # 1 Marker in at index 185 is also extra. We will remove it.\n",
    "\n",
    "            if (L_annt_r == 371):\n",
    "                Markers=Markers_Raw[1:-9] # :- Extras = 1 start and 9 end\n",
    "                index1 = 185 # index_to_remove\n",
    "                Markers = np.delete(Markers, index1)\n",
    "\n",
    "            # Subject 7 -- Session 1 -- X1 :-\n",
    "            ## This process is described and done in detail in another notebook.\n",
    "            # there are total 371 markers instead of 362.\n",
    "            # 1 marker from start and 1 marker from end are extra markers.\n",
    "            # 2 Markers at index 116 and index 180 are missing. We will insert these markers.\n",
    "\n",
    "\n",
    "            if (L_annt_r == 360):\n",
    "                Markers=Markers_Raw[1:-1] # :- Extras = 1 start and 1 end\n",
    "\n",
    "                index1 = 116\n",
    "                value1 = Markers[index1 - 1] + 25\n",
    "\n",
    "                index2 = 180\n",
    "                value2 = Markers[index2 - 1] + 25\n",
    "\n",
    "                insert_indices = [index1, index2]\n",
    "                values_to_insert = [value1, value2]\n",
    "\n",
    "                Markers = np.insert(Markers, insert_indices, values_to_insert)    \n",
    "\n",
    "            # Subject 8 -- Session 1 -- X4 :-\n",
    "            ## This process is described and done in detail in another notebook.\n",
    "            # there are total 363 markers instead of 362.\n",
    "            # 1 marker from start and 9 markers from end are extra markers.\n",
    "            # 1 Marker in at index 200 is also extra. We will remove it.\n",
    "\n",
    "            if (L_annt_r == 363):\n",
    "                Markers=Markers_Raw[1:-1] # :- Extras = 1 start and 1 end\n",
    "                index1 = 200 # index_to_remove\n",
    "                Markers = np.delete(Markers, index1)\n",
    "\n",
    "            # Subject 7 -- Session 1 -- X8 :-\n",
    "            ## This process is described and done in detail in another notebook.\n",
    "            # there are total 359 markers instead of 362.\n",
    "            # 1 marker from start and 1 marker from end are extra markers.\n",
    "            # 3 Markers at index 49, 50 and index 139 are missing. We will insert these markers.\n",
    "\n",
    "\n",
    "            if (L_annt_r == 359):\n",
    "                Markers=Markers_Raw[1:-1] # :- Extras = 1 start and 1 end\n",
    "\n",
    "                index1 = 49\n",
    "                value1 = Markers[index1 - 1] + 25\n",
    "\n",
    "                index2 = 50\n",
    "                value2 = Markers[index2 - 1] + 25\n",
    "\n",
    "                index3 = 139\n",
    "                value3 = Markers[index3 - 1] + 25\n",
    "\n",
    "                insert_indices = [index1, index2, index3]\n",
    "                values_to_insert = [value1, value2, value3]\n",
    "\n",
    "                Markers = np.insert(Markers, insert_indices, values_to_insert)     \n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"Unknown length\")\n",
    "            L_annt0=len(Markers)\n",
    "            print(\"Number of markers: \",L_annt0)\n",
    "\n",
    "            Zeros = [0] * Samples\n",
    "            Zero_arr = np.array(Zeros)\n",
    "            Zero_2d = np.reshape(Zero_arr, (len(Zero_arr), 1))\n",
    "            Zero_2dt=np.transpose(Zero_2d)\n",
    "\n",
    "            #csv_path= file_path.replace('Data', 'Markers').replace('.edf', '.csv')\n",
    "            csv_mark0 = pd.read_csv(csv_file_path, header=None)\n",
    "            csv_mark = np.array(csv_mark0)\n",
    "\n",
    "            zero_mark=Zero_arr\n",
    "            csv1=0\n",
    "            for x in Markers:   \n",
    "                zero_mark[x]=csv_mark[csv1]\n",
    "                csv1=csv1+1\n",
    "\n",
    "            tar = np.where(zero_mark == 1)\n",
    "            std = np.where(zero_mark == 2)\n",
    "\n",
    "            new_events = []\n",
    "            for st in std[0]:\n",
    "                new_events.append([st, 0, 2]) \n",
    "            for t in tar[0]:\n",
    "                new_events.append([t, 0, 1])\n",
    "\n",
    "            event_dict = {'target':1, 'standard':2}\n",
    "            epoch = mne.Epochs(raw, new_events, tmin=-0.2, tmax=0.8, event_id=event_dict,\n",
    "                            preload=True, baseline=None)\n",
    "\n",
    "            #----------------------------------------\n",
    "\n",
    "            evoked_target = epoch['target'].average()\n",
    "            evoked_standard = epoch['standard'].average()\n",
    "            evoked_diff = mne.combine_evoked([evoked_target, evoked_standard], weights=[1, -1])\n",
    "            #----------------------------------------------\n",
    "\n",
    "     \n",
    "            return epoch\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        # Only RSVP Tasks ( X1 and X2 )\n",
    "        # Clean ---> c\n",
    "        # There are two X1s and two X2s in each session\n",
    "\n",
    "        #----------------------------------------Session-1----------------------------------------\n",
    "\n",
    "        # Raw EEG edf file path\n",
    "        edf_path_c1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Data\\\\P{}-Ss1-X1-eeg.edf'.format(P_Num, P_Num)\n",
    "\n",
    "        # RSVP image labels file path\n",
    "        csv_path_c1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Mark\\\\P{}-Ss1-X1-mrk.csv'.format(P_Num, P_Num)\n",
    "\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c1 = os.path.basename(edf_path_c1).split('.')[0]\n",
    "        epoch_c1 = process_file(edf_path_c1,csv_path_c1, filename_c1)\n",
    "\n",
    "\n",
    "\n",
    "        edf_path_c2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Data\\\\P{}-Ss1-X2-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_c2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Mark\\\\P{}-Ss1-X2-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c2 = os.path.basename(edf_path_c2).split('.')[0]\n",
    "        epoch_c2 = process_file(edf_path_c2,csv_path_c2, filename_c2)\n",
    "\n",
    "        #----------------------------------------Session-2----------------------------------------\n",
    "\n",
    "        edf_path_c3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Data\\\\P{}-Ss2-X1-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_c3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Mark\\\\P{}-Ss2-X1-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c3 = os.path.basename(edf_path_c3).split('.')[0]\n",
    "        epoch_c3 = process_file(edf_path_c3,csv_path_c3, filename_c3)\n",
    "\n",
    "        edf_path_c4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Data\\\\P{}-Ss2-X2-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_c4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Mark\\\\P{}-Ss2-X2-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c4 = os.path.basename(edf_path_c4).split('.')[0]\n",
    "        epoch_c4 = process_file(edf_path_c4,csv_path_c4, filename_c4)\n",
    "\n",
    "        #----------------------------------------Session-3----------------------------------------\n",
    "\n",
    "        edf_path_c5 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Data\\\\P{}-Ss3-X1-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_c5 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Mark\\\\P{}-Ss3-X1-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c5 = os.path.basename(edf_path_c5).split('.')[0]\n",
    "        epoch_c5 = process_file(edf_path_c5,csv_path_c5, filename_c5)\n",
    "\n",
    "        edf_path_c6 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Data\\\\P{}-Ss3-X2-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_c6 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Mark\\\\P{}-Ss3-X2-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c6 = os.path.basename(edf_path_c6).split('.')[0]\n",
    "        epoch_c6 = process_file(edf_path_c6,csv_path_c6, filename_c6)\n",
    "\n",
    "        #----------------------------------------Session-4----------------------------------------\n",
    "\n",
    "        edf_path_c7 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Data\\\\P{}-Ss4-X1-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_c7 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Mark\\\\P{}-Ss4-X1-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c7 = os.path.basename(edf_path_c7).split('.')[0]\n",
    "        epoch_c7 = process_file(edf_path_c7,csv_path_c7, filename_c7)\n",
    "\n",
    "        edf_path_c8 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Data\\\\P{}-Ss4-X2-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_c8 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Mark\\\\P{}-Ss4-X2-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c8 = os.path.basename(edf_path_c8).split('.')[0]\n",
    "        epoch_c8 = process_file(edf_path_c8,csv_path_c8, filename_c8)\n",
    " \n",
    "        # Batch 1 training purposes\n",
    "        Epoch_C_Tr = mne.concatenate_epochs([epoch_c1, epoch_c2, epoch_c4, epoch_c5, epoch_c7, epoch_c8])\n",
    "        Epoch_C_Ts = mne.concatenate_epochs([epoch_c3, epoch_c6])\n",
    "\n",
    "\n",
    "\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "        # RSVP with Body Movement ( X4 )\n",
    "        # Body ---> b\n",
    "\n",
    "        #----------------------------------------Session-1----------------------------------------\n",
    "\n",
    "        # Raw EEG edf file path\n",
    "        edf_path_b1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Data\\\\P{}-Ss1-X4-eeg.edf'.format(P_Num, P_Num)\n",
    "\n",
    "        # RSVP image labels file path\n",
    "        csv_path_b1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Mark\\\\P{}-Ss1-X4-mrk.csv'.format(P_Num, P_Num)\n",
    "\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_b1 = os.path.basename(edf_path_b1).split('.')[0]\n",
    "        epoch_b1 = process_file(edf_path_b1,csv_path_b1, filename_b1)\n",
    "\n",
    "\n",
    "        #----------------------------------------Session-2----------------------------------------\n",
    "\n",
    "        edf_path_b2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Data\\\\P{}-Ss2-X4-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_b2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Mark\\\\P{}-Ss2-X4-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_b2 = os.path.basename(edf_path_b2).split('.')[0]\n",
    "        epoch_b2 = process_file(edf_path_b2,csv_path_b2, filename_b2)\n",
    "\n",
    "        #----------------------------------------Session-3----------------------------------------\n",
    "\n",
    "        edf_path_b3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Data\\\\P{}-Ss3-X4-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_b3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Mark\\\\P{}-Ss3-X4-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_b3 = os.path.basename(edf_path_b3).split('.')[0]\n",
    "        epoch_b3 = process_file(edf_path_b3,csv_path_b3, filename_b3)\n",
    "\n",
    "        #----------------------------------------Session-4----------------------------------------\n",
    "\n",
    "        edf_path_b4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Data\\\\P{}-Ss4-X4-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_b4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Mark\\\\P{}-Ss4-X4-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_b4 = os.path.basename(edf_path_b4).split('.')[0]\n",
    "        epoch_b4 = process_file(edf_path_b4,csv_path_b4, filename_b4)\n",
    "\n",
    "        #------------------\n",
    "        Epoch_B = mne.concatenate_epochs([epoch_b1, epoch_b2, epoch_b3, epoch_b4])\n",
    "\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "        # RSVP with Talking ( X6 )\n",
    "        # Talking ---> t\n",
    "\n",
    "        #----------------------------------------Session-1----------------------------------------\n",
    "\n",
    "        # Raw EEG edf file path\n",
    "        edf_path_t1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Data\\\\P{}-Ss1-X6-eeg.edf'.format(P_Num, P_Num)\n",
    "\n",
    "        # RSVP image labels file path\n",
    "        csv_path_t1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Mark\\\\P{}-Ss1-X6-mrk.csv'.format(P_Num, P_Num)\n",
    "\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_t1 = os.path.basename(edf_path_t1).split('.')[0]\n",
    "        epoch_t1 = process_file(edf_path_t1,csv_path_t1, filename_t1)\n",
    "\n",
    "\n",
    "        #----------------------------------------Session-2----------------------------------------\n",
    "\n",
    "        edf_path_t2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Data\\\\P{}-Ss2-X6-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_t2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Mark\\\\P{}-Ss2-X6-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_t2 = os.path.basename(edf_path_t2).split('.')[0]\n",
    "        epoch_t2 = process_file(edf_path_t2,csv_path_t2, filename_t2)\n",
    "\n",
    "        #----------------------------------------Session-3----------------------------------------\n",
    "\n",
    "        edf_path_t3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Data\\\\P{}-Ss3-X6-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_t3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Mark\\\\P{}-Ss3-X6-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_t3 = os.path.basename(edf_path_t3).split('.')[0]\n",
    "        epoch_t3 = process_file(edf_path_t3,csv_path_t3, filename_t3)\n",
    "\n",
    "        #----------------------------------------Session-4----------------------------------------\n",
    "\n",
    "        edf_path_t4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Data\\\\P{}-Ss4-X6-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_t4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Mark\\\\P{}-Ss4-X6-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_t4 = os.path.basename(edf_path_t4).split('.')[0]\n",
    "        epoch_t4 = process_file(edf_path_t4,csv_path_t4, filename_t4)\n",
    "\n",
    "        #------------------\n",
    "        Epoch_T = mne.concatenate_epochs([epoch_t1, epoch_t2, epoch_t3 ,epoch_t4])\n",
    "\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        # RSVP with Head Movement ( X8 )\n",
    "        # Head ---> h\n",
    "\n",
    "        #----------------------------------------Session-1----------------------------------------\n",
    "\n",
    "        # Raw EEG edf file path\n",
    "        edf_path_h1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Data\\\\P{}-Ss1-X8-eeg.edf'.format(P_Num, P_Num)\n",
    "\n",
    "        # RSVP image labels file path\n",
    "        csv_path_h1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Mark\\\\P{}-Ss1-X8-mrk.csv'.format(P_Num, P_Num)\n",
    "\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_h1 = os.path.basename(edf_path_h1).split('.')[0]\n",
    "        epoch_h1 = process_file(edf_path_h1,csv_path_h1, filename_h1)\n",
    "\n",
    "\n",
    "        #----------------------------------------Session-2----------------------------------------\n",
    "\n",
    "        edf_path_h2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Data\\\\P{}-Ss2-X8-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_h2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Mark\\\\P{}-Ss2-X8-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_h2 = os.path.basename(edf_path_h2).split('.')[0]\n",
    "        epoch_h2 = process_file(edf_path_h2,csv_path_h2, filename_h2)\n",
    "\n",
    "        #----------------------------------------Session-3----------------------------------------\n",
    "\n",
    "        edf_path_h3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Data\\\\P{}-Ss3-X8-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_h3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Mark\\\\P{}-Ss3-X8-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_h3 = os.path.basename(edf_path_h3).split('.')[0]\n",
    "        epoch_h3 = process_file(edf_path_h3,csv_path_h3, filename_h3)\n",
    "\n",
    "        #----------------------------------------Session-4----------------------------------------\n",
    "\n",
    "        edf_path_h4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Data\\\\P{}-Ss4-X8-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_h4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Mark\\\\P{}-Ss4-X8-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_h4 = os.path.basename(edf_path_h4).split('.')[0]\n",
    "        epoch_h4 = process_file(edf_path_h4,csv_path_h4, filename_h4)\n",
    "\n",
    "        #------------------\n",
    "        Epoch_H = mne.concatenate_epochs([epoch_h1,epoch_h2, epoch_h3, epoch_h4])\n",
    "        \n",
    "        \n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------       \n",
    "        #THR=100e-6 #Threshold\n",
    "        # Set your peak-to-peak threshold\n",
    "        p2p_threshold_C = THR  # #you can change it.. may be 100e-6 as well\n",
    "        p2p_threshold_B = THR  # #you can change it.. may be 100e-6 as well\n",
    "        p2p_threshold_T = THR  # #you can change it.. may be 100e-6 as well\n",
    "        p2p_threshold_H = THR  # #you can change it.. may be 100e-6 as well\n",
    "\n",
    "        # Make the copy of epochs\n",
    "        Epoch_C_All=Epoch_C_Tr.copy()\n",
    "        Epoch_B_All=Epoch_B.copy()\n",
    "        Epoch_T_All=Epoch_T.copy()\n",
    "        Epoch_H_All=Epoch_H.copy()\n",
    "        \n",
    "        # Get the data from epochs\n",
    "        Epoch_C_All_Data = Epoch_C_All.get_data()\n",
    "        Epoch_B_All_Data = Epoch_B_All.get_data()\n",
    "        Epoch_T_All_Data = Epoch_T_All.get_data()\n",
    "        Epoch_H_All_Data = Epoch_H_All.get_data()\n",
    "    \n",
    "\n",
    "        # Calculate the peak-to-peak amplitude along the time axis\n",
    "        p2p_amplitude_C = np.ptp(Epoch_C_All_Data, axis=2)\n",
    "        p2p_amplitude_B = np.ptp(Epoch_B_All_Data, axis=2)\n",
    "        p2p_amplitude_T = np.ptp(Epoch_T_All_Data, axis=2)\n",
    "        p2p_amplitude_H = np.ptp(Epoch_H_All_Data, axis=2)\n",
    "        \n",
    "\n",
    "        # Find epochs that exceed the peak-to-peak threshold\n",
    "        bad_epochs_C = np.where(p2p_amplitude_C > p2p_threshold_C)[0]\n",
    "        bad_epochs_B = np.where(p2p_amplitude_B > p2p_threshold_B)[0]\n",
    "        bad_epochs_T = np.where(p2p_amplitude_T > p2p_threshold_T)[0]\n",
    "        bad_epochs_H = np.where(p2p_amplitude_H > p2p_threshold_H)[0]\n",
    "        \n",
    "\n",
    "        # Clean Epochs :- Drop bad epochs from the Epochs object\n",
    "        Clean_epochs_C = Epoch_C_All.copy()\n",
    "        Clean_epochs_C.drop(bad_epochs_C)\n",
    "        \n",
    "        Clean_epochs_B = Epoch_B_All.copy()\n",
    "        Clean_epochs_B.drop(bad_epochs_B)\n",
    "        \n",
    "        Clean_epochs_T = Epoch_T_All.copy()\n",
    "        Clean_epochs_T.drop(bad_epochs_T)\n",
    "        \n",
    "        Clean_epochs_H = Epoch_H_All.copy()\n",
    "        Clean_epochs_H.drop(bad_epochs_H)\n",
    "\n",
    "        # Rejected Epochs \n",
    "        \n",
    "        Clean_indices_C = set(Clean_epochs_C.selection)                # Find the indices of clean epochs\n",
    "        All_indices_C = set(Epoch_C_All.selection)                     # Find the indices of all epochs\n",
    "        Rejected_indices_C = All_indices_C - Clean_indices_C           # Find the indices of rejected epochs\n",
    "        Rejected_indices_C = list(Rejected_indices_C)                  # making a list of indices\n",
    "        Rejected_indices_C = np.array(Rejected_indices_C).astype(int)  # Converting the list of indices to int\n",
    "        Rejected_epochs_C = Epoch_C_All[Rejected_indices_C]\n",
    "        \n",
    "        Clean_indices_B = set(Clean_epochs_B.selection)                # Find the indices of clean epochs\n",
    "        All_indices_B = set(Epoch_B_All.selection)                     # Find the indices of all epochs\n",
    "        Rejected_indices_B = All_indices_B - Clean_indices_B           # Find the indices of rejected epochs\n",
    "        Rejected_indices_B = list(Rejected_indices_B)                  # making a list of indices\n",
    "        Rejected_indices_B = np.array(Rejected_indices_B).astype(int)  # Converting the list of indices to int\n",
    "        Rejected_epochs_B = Epoch_B_All[Rejected_indices_B] \n",
    "        \n",
    "        Clean_indices_T = set(Clean_epochs_T.selection)                # Find the indices of clean epochs\n",
    "        All_indices_T = set(Epoch_T_All.selection)                     # Find the indices of all epochs\n",
    "        Rejected_indices_T = All_indices_T - Clean_indices_T           # Find the indices of rejected epochs\n",
    "        Rejected_indices_T = list(Rejected_indices_T)                  # making a list of indices\n",
    "        Rejected_indices_T = np.array(Rejected_indices_T).astype(int)  # Converting the list of indices to int\n",
    "        Rejected_epochs_T = Epoch_T_All[Rejected_indices_T]\n",
    "        \n",
    "        Clean_indices_H = set(Clean_epochs_H.selection)                # Find the indices of clean epochs\n",
    "        All_indices_H = set(Epoch_H_All.selection)                     # Find the indices of all epochs\n",
    "        Rejected_indices_H = All_indices_H - Clean_indices_H           # Find the indices of rejected epochs\n",
    "        Rejected_indices_H = list(Rejected_indices_H)                  # making a list of indices\n",
    "        Rejected_indices_H = np.array(Rejected_indices_H).astype(int)  # Converting the list of indices to int\n",
    "        Rejected_epochs_H = Epoch_H_All[Rejected_indices_H]\n",
    "        \n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------    \n",
    "        \n",
    "        # Getting the percentage of dropped/bad epochs\n",
    "\n",
    "        percentage_dropped_C=(len(Rejected_epochs_C) / len(Epoch_C_All)) * 100\n",
    "        \n",
    "        percentage_dropped_B=(len(Rejected_epochs_B) / len(Epoch_B_All)) * 100\n",
    "        \n",
    "        percentage_dropped_T=(len(Rejected_epochs_T) / len(Epoch_T_All)) * 100   \n",
    "        \n",
    "        percentage_dropped_H=(len(Rejected_epochs_H) / len(Epoch_H_All)) * 100\n",
    "        \n",
    "        \n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------   \n",
    "        \n",
    "        # Traditional RSVP Clean Epochs for Training\n",
    "        Data_C = Clean_epochs_C.get_data()\n",
    "        Data_C = Data_C.reshape(Data_C.shape[0], -1)\n",
    "        Events_C = Clean_epochs_C.events[:, -1]\n",
    "\n",
    "        # Traditional RSVP Test Epochs for Testing (mix-clean and bad) # This set was seperated at start :- Epoch 3 and 6 only\n",
    "        if percentage_dropped_C < 80 :\n",
    "            Data_C_All = Epoch_C_Ts.get_data()\n",
    "            Data_C_All = Data_C_All.reshape(Data_C_All.shape[0], -1)\n",
    "            Events_C_All = Epoch_C_Ts.events[:, -1]\n",
    "\n",
    "        else:\n",
    "            Data_C_All = 0\n",
    "            Events_C_All = 0\n",
    "            print(\"Percentage of dropped epochs-C is greater than 80\")\n",
    "        \n",
    "        # Traditional RSVP Rejected Epochs for Testing\n",
    "        if percentage_dropped_C > 0 :\n",
    "            Data_C_Rejected = Rejected_epochs_C.get_data()\n",
    "            Data_C_Rejected = Data_C_Rejected.reshape(Data_C_Rejected.shape[0], -1)\n",
    "            Events_C_Rejected = Rejected_epochs_C.events[:, -1]\n",
    "        else:\n",
    "            Data_C_Rejected = 0\n",
    "            Events_C_Rejected = 0\n",
    "            print(\"Percentage of dropped epochs-C is 0\")\n",
    "              \n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # RSVP-Body all epochs for testing \n",
    "        Data_B_All = Epoch_B_All.get_data()\n",
    "        Data_B_All = Data_B_All.reshape(Data_B_All.shape[0], -1)\n",
    "        Events_B_All = Epoch_B_All.events[:, -1]\n",
    "\n",
    "        # RSVP-Body clean epochs for testing\n",
    "        if percentage_dropped_B < 80 :\n",
    "            Data_B_Clean = Clean_epochs_B.get_data()\n",
    "            Data_B_Clean = Data_B_Clean.reshape(Data_B_Clean.shape[0], -1)\n",
    "            Events_B_Clean = Clean_epochs_B.events[:, -1]     \n",
    "        else:\n",
    "            Data_B_Clean = 0\n",
    "            Events_B_Clean = 0\n",
    "            print(\"Percentage of dropped epochs-B is greater than 80\")\n",
    "            \n",
    "        # RSVP-Body rejected epochs for testing\n",
    "        if percentage_dropped_B > 0 :\n",
    "            Data_B_Rejected = Rejected_epochs_B.get_data()\n",
    "            Data_B_Rejected = Data_B_Rejected.reshape(Data_B_Rejected.shape[0], -1)\n",
    "            Events_B_Rejected = Rejected_epochs_B.events[:, -1]\n",
    "        else:\n",
    "            Data_B_Rejected = 0\n",
    "            Events_B_Rejected = 0\n",
    "            print(\"Percentage of dropped epochs-B is 0\")\n",
    "            \n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # RSVP-Talk all epochs for testing \n",
    "        Data_T_All = Epoch_T_All.get_data()\n",
    "        Data_T_All = Data_T_All.reshape(Data_T_All.shape[0], -1)\n",
    "        Events_T_All = Epoch_T_All.events[:, -1]\n",
    "\n",
    "        # RSVP-Talk clean epochs for testing\n",
    "        if percentage_dropped_T < 80:\n",
    "            Data_T_Clean = Clean_epochs_T.get_data()\n",
    "            Data_T_Clean = Data_T_Clean.reshape(Data_T_Clean.shape[0], -1)\n",
    "            Events_T_Clean = Clean_epochs_T.events[:, -1]\n",
    "        else:\n",
    "            Data_T_Clean = 0\n",
    "            Events_T_Clean = 0\n",
    "            print(\"Percentage of dropped epochs-T is greater than 80\")\n",
    "        \n",
    "        # RSVP-Talk rejected epochs for testing\n",
    "        if percentage_dropped_T > 0 :\n",
    "            Data_T_Rejected = Rejected_epochs_T.get_data()\n",
    "            Data_T_Rejected = Data_T_Rejected.reshape(Data_T_Rejected.shape[0], -1)\n",
    "            Events_T_Rejected = Rejected_epochs_T.events[:, -1]\n",
    "        else:\n",
    "            Data_T_Rejected = 0\n",
    "            Events_T_Rejected = 0\n",
    "            print(\"Percentage of dropped epochs-T is 0\")\n",
    "        \n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # RSVP-Head all epochs for testing \n",
    "        Data_H_All = Epoch_H_All.get_data()\n",
    "        Data_H_All = Data_H_All.reshape(Data_H_All.shape[0], -1)\n",
    "        Events_H_All = Epoch_H_All.events[:, -1]\n",
    "\n",
    "        # RSVP-Head clean epochs for testing\n",
    "        if percentage_dropped_H < 80:\n",
    "            Data_H_Clean = Clean_epochs_H.get_data()\n",
    "            Data_H_Clean = Data_H_Clean.reshape(Data_H_Clean.shape[0], -1)\n",
    "            Events_H_Clean = Clean_epochs_H.events[:, -1]\n",
    "        else:\n",
    "            Data_H_Clean = 0\n",
    "            Events_H_Clean = 0\n",
    "            print(\"Percentage of dropped epochs-H is greater than 80\")\n",
    "        \n",
    "        # RSVP-Head rejected epochs for testing\n",
    "        if percentage_dropped_H > 0 :\n",
    "            Data_H_Rejected = Rejected_epochs_H.get_data()\n",
    "            Data_H_Rejected = Data_H_Rejected.reshape(Data_H_Rejected.shape[0], -1)\n",
    "            Events_H_Rejected = Rejected_epochs_H.events[:, -1]\n",
    "        else:\n",
    "            Data_H_Rejected = 0\n",
    "            Events_H_Rejected = 0\n",
    "            print(\"Percentage of dropped epochs-H is 0\")\n",
    "        \n",
    "       ##----------------------------------------------------------------------------------\n",
    "\n",
    "        ## Training\n",
    "\n",
    "            best_params = {\n",
    "            'alpha_1': 4.692680899768591e-06,\n",
    "            'alpha_2': 3.0101214309175212e-05,\n",
    "            'lambda_1': 1.3167456935454495e-05,\n",
    "            'lambda_2': 9.129425537759534e-06\n",
    "        }\n",
    "        scaler = StandardScaler() # Normalize the features\n",
    "        \n",
    "        # Create BayesianRidge model\n",
    "        mod = BayesianRidge(**best_params)\n",
    "        \n",
    "        # Create a pipeline with normalization and BayesianRidge\n",
    "        clf = make_pipeline(scaler, mod)\n",
    "\n",
    "        # Fit the model\n",
    "        clf.fit(Data_C, Events_C)\n",
    "\n",
    "        \n",
    "       \n",
    "        ##------------------------------------------------------------------------------------------     \n",
    "        \n",
    "        ## Testing Traditional RSVP\n",
    "\n",
    "        ## Traditional RSVP (All epochs :- Ep3 and Ep6)\n",
    "        y_pred_all_C = clf.predict(Data_C_All)\n",
    "        rocauc_all_C= roc_auc_score(Events_C_All, y_pred_all_C)\n",
    "        \n",
    "        ## Traditional RSVP (Clean epochs)\n",
    "        y_pred_clean_C = clf.predict(Data_C)\n",
    "        rocauc_clean_C= roc_auc_score(Events_C, y_pred_clean_C)\n",
    "        \n",
    "        ## Traditional RSVP (Rejected epochs)\n",
    "        if len(np.unique(Events_C_Rejected)) >1:\n",
    "            y_pred_rej_C = clf.predict(Data_C_Rejected)\n",
    "            rocauc_rej_C= roc_auc_score(Events_C_Rejected, y_pred_rej_C)\n",
    "        else:\n",
    "            rocauc_rej_C=0\n",
    "            print(\"Error: There is only one class present\")\n",
    "        \n",
    "        print(P_Num, 'RSVP-Traditional')\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        \n",
    "        ## Testing Body RSVP\n",
    "        \n",
    "        ## Body RSVP (All epochs :- Ep3 and Ep6)\n",
    "        y_pred_all_B = clf.predict(Data_B_All)\n",
    "        rocauc_all_B= roc_auc_score(Events_B_All, y_pred_all_B)\n",
    "        \n",
    "        ## Body  RSVP (Clean epochs)\n",
    "        if percentage_dropped_B < 80:\n",
    "            y_pred_clean_B = clf.predict(Data_B_Clean)\n",
    "            rocauc_clean_B= roc_auc_score(Events_B_Clean, y_pred_clean_B)\n",
    "        else:\n",
    "            rocauc_clean_B=0\n",
    "            print(\"Cannot Classify: Percentage of dropped epochs-B is greater than 80\")\n",
    "        \n",
    "        ## Body RSVP (Rejected epochs)\n",
    "        if len(np.unique(Events_B_Rejected)) >1:\n",
    "            y_pred_rej_B = clf.predict(Data_B_Rejected)\n",
    "            rocauc_rej_B= roc_auc_score(Events_B_Rejected, y_pred_rej_B)\n",
    "\n",
    "        else:\n",
    "            rocauc_rej_B=0\n",
    "            print(\"Cannot Classify: single class present\")\n",
    "        \n",
    "        print(P_Num, 'RSVP-Body')\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        \n",
    "        ## Testing Talking RSVP\n",
    "        \n",
    "        ## Talking RSVP (All epochs :- Ep3 and Ep6)\n",
    "        y_pred_all_T = clf.predict(Data_T_All)\n",
    "        rocauc_all_T= roc_auc_score(Events_T_All, y_pred_all_T)\n",
    "        \n",
    "        ## Talking RSVP (Clean epochs)\n",
    "        if percentage_dropped_T < 80:\n",
    "            y_pred_clean_T = clf.predict(Data_T_Clean)\n",
    "            rocauc_clean_T= roc_auc_score(Events_T_Clean, y_pred_clean_T)\n",
    "        else:\n",
    "            rocauc_clean_T=0\n",
    "            print(\"Cannot Classify: Percentage of dropped epochs-T is greater than 80\")\n",
    "        \n",
    "        ## Talking RSVP (Rejected epochs)\n",
    "        if len(np.unique(Events_T_Rejected)) >1:\n",
    "            y_pred_rej_T = clf.predict(Data_T_Rejected)\n",
    "            rocauc_rej_T= roc_auc_score(Events_T_Rejected, y_pred_rej_T)\n",
    "                  \n",
    "        else:\n",
    "            rocauc_rej_T=0\n",
    "            print(\"Cannot Classify: single class present\")\n",
    "        \n",
    "        print(P_Num, 'RSVP-Taking')\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        \n",
    "        ## Testing Head RSVP\n",
    "        \n",
    "        ## Head RSVP (All epochs :- Ep3 and Ep6)\n",
    "        y_pred_all_H = clf.predict(Data_H_All)\n",
    "        rocauc_all_H= roc_auc_score(Events_H_All, y_pred_all_H)\n",
    "        \n",
    "        ## Head  RSVP (Clean epochs)\n",
    "        if percentage_dropped_H < 80:\n",
    "            y_pred_clean_H = clf.predict(Data_H_Clean)\n",
    "            rocauc_clean_H= roc_auc_score(Events_H_Clean, y_pred_clean_H)\n",
    "        else:\n",
    "            rocauc_clean_H=0\n",
    "            print(\"Cannot Classify: Percentage of dropped epochs-H is greater than 80\")\n",
    "        \n",
    "        ## Head RSVP (Rejected epochs)\n",
    "        if len(np.unique(Events_H_Rejected)) >1:\n",
    "            y_pred_rej_H = clf.predict(Data_H_Rejected)\n",
    "            rocauc_rej_H= roc_auc_score(Events_H_Rejected, y_pred_rej_H)\n",
    "        else:\n",
    "            rocauc_rej_H=0\n",
    "            print(\"Cannot Classify: single class present\")                      \n",
    "\n",
    "        \n",
    "        print(P_Num, 'RSVP-Head')\n",
    "       \n",
    "\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "        X=00000000000.0 #  Just to add extrac column in csv file\n",
    "        \n",
    "        # Write the values to the CSV file\n",
    "        writer.writerow([P_Num, round(rocauc_all_C, 2), round(rocauc_clean_C, 2), round(rocauc_rej_C, 2), X, \n",
    "                         round(rocauc_all_B, 2), round(rocauc_clean_B, 2), round(rocauc_rej_B, 2), X,\n",
    "                        round(rocauc_all_T, 2), round(rocauc_clean_T, 2), round(rocauc_rej_T, 2), X,\n",
    "                        round(rocauc_all_H, 2), round(rocauc_clean_H, 2), round(rocauc_rej_H, 2)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225350db-9f16-4a90-866c-7da78f604fa1",
   "metadata": {},
   "source": [
    "**Table 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb89bb-a414-4c35-852f-86fde765cea9",
   "metadata": {},
   "source": [
    "**Model trained and tested** on different conditions including only clean trials for training or testing and then training or testing the model using a combination of clean and bad trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76384c9c-b6ab-4a29-83d8-45ed909da0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "THR=100e-6 # P2P threshold for trial rejection \n",
    "# Open a CSV file for writing\n",
    "with open('RocAuc_TrainClean_&_TrainNoise_100uv_06Feb.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['P_Num', 'NIC-Clean', 'XXXXXXXXXXX', 'NIC-Clean+Noise', 'XXXXXXXXXXX', 'IC-Clean', 'XXXXXXXXXXX', 'IC-Clean+Noise', 'XXXXXXXXXXX', 'Train-IC-Clean-Test-NIC-Clean', 'XXXXXXXXXXX', 'Train-IC-Clean+Noise-Test-NIC-Clean+Noise'])\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        P_Num = i\n",
    "        \n",
    "        \n",
    "        \n",
    "        #P_Num=10\n",
    "\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "        def process_file(edf_file_path, csv_file_path, filename):\n",
    "            raw=mne.io.read_raw_edf(edf_file_path, preload=True)\n",
    "\n",
    "            # Pick 6 out of 32 channels\n",
    "            #channel_names_to_select = ['Fz-CPz','Cz-CPz', 'P3-CPz', 'Pz-CPz', 'P4-CPz','Oz-CPz']\n",
    "            #raw.pick_channels(channel_names_to_select)\n",
    "\n",
    "            #Applying lowpass filter at 30Hz\n",
    "            raw = raw.filter(.1, 30, method='iir')\n",
    "\n",
    "            # Resampling data from 1000sps to 100sps\n",
    "            raw = raw.resample(sfreq=100)\n",
    "\n",
    "\n",
    "            #Re-Referencing the data to common average reference (CAR)\n",
    "            raw= raw.set_eeg_reference(ref_channels='average')\n",
    "\n",
    "\n",
    "            data = raw.get_data()\n",
    "            Samples=len(data[0])\n",
    "\n",
    "            annt=mne.events_from_annotations(raw, event_id='auto', regexp='^(?![Bb][Aa][Dd]|[Ee][Dd][Gg][Ee]).*$', use_rounding=True, chunk_duration=None, verbose=None)\n",
    "            ann0=annt[0]\n",
    "            Markers_Raw=ann0[:,0]\n",
    "            L_annt_r=len(Markers_Raw)\n",
    "\n",
    "            if (L_annt_r == 362):\n",
    "                Markers=Markers_Raw[1:-1] # :- Extras = 1 start and 1 end\n",
    "\n",
    "\n",
    "            # Subject 5 -- Session 4 -- X1 :-\n",
    "            ## This process is described and done in detail in another notebook.\n",
    "            # there are total 371 markers instead of 362.\n",
    "            # 1 marker from start and 9 markers from end are extra markers.\n",
    "            # 1 Marker in at index 185 is also extra. We will remove it.\n",
    "\n",
    "            if (L_annt_r == 371):\n",
    "                Markers=Markers_Raw[1:-9] # :- Extras = 1 start and 9 end\n",
    "                index1 = 185 # index_to_remove\n",
    "                Markers = np.delete(Markers, index1)\n",
    "\n",
    "            # Subject 7 -- Session 1 -- X1 :-\n",
    "            ## This process is described and done in detail in another notebook.\n",
    "            # there are total 371 markers instead of 362.\n",
    "            # 1 marker from start and 1 marker from end are extra markers.\n",
    "            # 2 Markers at index 116 and index 180 are missing. We will insert these markers.\n",
    "\n",
    "\n",
    "            if (L_annt_r == 360):\n",
    "                Markers=Markers_Raw[1:-1] # :- Extras = 1 start and 1 end\n",
    "\n",
    "                index1 = 116\n",
    "                value1 = Markers[index1 - 1] + 25\n",
    "\n",
    "                index2 = 180\n",
    "                value2 = Markers[index2 - 1] + 25\n",
    "\n",
    "                insert_indices = [index1, index2]\n",
    "                values_to_insert = [value1, value2]\n",
    "\n",
    "                Markers = np.insert(Markers, insert_indices, values_to_insert)    \n",
    "\n",
    "            # Subject 8 -- Session 1 -- X4 :-\n",
    "            ## This process is described and done in detail in another notebook.\n",
    "            # there are total 363 markers instead of 362.\n",
    "            # 1 marker from start and 9 markers from end are extra markers.\n",
    "            # 1 Marker in at index 200 is also extra. We will remove it.\n",
    "\n",
    "            if (L_annt_r == 363):\n",
    "                Markers=Markers_Raw[1:-1] # :- Extras = 1 start and 1 end\n",
    "                index1 = 200 # index_to_remove\n",
    "                Markers = np.delete(Markers, index1)\n",
    "\n",
    "            # Subject 7 -- Session 1 -- X8 :-\n",
    "            ## This process is described and done in detail in another notebook.\n",
    "            # there are total 359 markers instead of 362.\n",
    "            # 1 marker from start and 1 marker from end are extra markers.\n",
    "            # 3 Markers at index 49, 50 and index 139 are missing. We will insert these markers.\n",
    "\n",
    "\n",
    "            if (L_annt_r == 359):\n",
    "                Markers=Markers_Raw[1:-1] # :- Extras = 1 start and 1 end\n",
    "\n",
    "                index1 = 49\n",
    "                value1 = Markers[index1 - 1] + 25\n",
    "\n",
    "                index2 = 50\n",
    "                value2 = Markers[index2 - 1] + 25\n",
    "\n",
    "                index3 = 139\n",
    "                value3 = Markers[index3 - 1] + 25\n",
    "\n",
    "                insert_indices = [index1, index2, index3]\n",
    "                values_to_insert = [value1, value2, value3]\n",
    "\n",
    "                Markers = np.insert(Markers, insert_indices, values_to_insert)     \n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"Unknown length\")\n",
    "            L_annt0=len(Markers)\n",
    "            print(\"Number of markers: \",L_annt0)\n",
    "\n",
    "            Zeros = [0] * Samples\n",
    "            Zero_arr = np.array(Zeros)\n",
    "            Zero_2d = np.reshape(Zero_arr, (len(Zero_arr), 1))\n",
    "            Zero_2dt=np.transpose(Zero_2d)\n",
    "\n",
    "            #csv_path= file_path.replace('Data', 'Markers').replace('.edf', '.csv')\n",
    "            csv_mark0 = pd.read_csv(csv_file_path, header=None)\n",
    "            csv_mark = np.array(csv_mark0)\n",
    "\n",
    "            zero_mark=Zero_arr\n",
    "            csv1=0\n",
    "            for x in Markers:   \n",
    "                zero_mark[x]=csv_mark[csv1]\n",
    "                csv1=csv1+1\n",
    "\n",
    "            tar = np.where(zero_mark == 1)\n",
    "            std = np.where(zero_mark == 2)\n",
    "\n",
    "            new_events = []\n",
    "            for st in std[0]:\n",
    "                new_events.append([st, 0, 2]) \n",
    "            for t in tar[0]:\n",
    "                new_events.append([t, 0, 1])\n",
    "\n",
    "            event_dict = {'target':1, 'standard':2}\n",
    "            epoch = mne.Epochs(raw, new_events, tmin=-0.2, tmax=0.8, event_id=event_dict,\n",
    "                            preload=True, baseline=None)\n",
    "\n",
    "            #----------------------------------------\n",
    "\n",
    "            evoked_target = epoch['target'].average()\n",
    "            evoked_standard = epoch['standard'].average()\n",
    "            evoked_diff = mne.combine_evoked([evoked_target, evoked_standard], weights=[1, -1])\n",
    "            #----------------------------------------------\n",
    "\n",
    "     \n",
    "            return epoch\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        # Only RSVP Tasks ( X1 and X2 )\n",
    "        # Clean ---> c\n",
    "        # There are two X1s and two X2s in each session\n",
    "\n",
    "        #----------------------------------------Session-1----------------------------------------\n",
    "\n",
    "        # Raw EEG edf file path\n",
    "        edf_path_c1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Data\\\\P{}-Ss1-X1-eeg.edf'.format(P_Num, P_Num)\n",
    "\n",
    "        # RSVP image labels file path\n",
    "        csv_path_c1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Mark\\\\P{}-Ss1-X1-mrk.csv'.format(P_Num, P_Num)\n",
    "\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c1 = os.path.basename(edf_path_c1).split('.')[0]\n",
    "        epoch_c1 = process_file(edf_path_c1,csv_path_c1, filename_c1)\n",
    "\n",
    "\n",
    "\n",
    "        edf_path_c2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Data\\\\P{}-Ss1-X2-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_c2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Mark\\\\P{}-Ss1-X2-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c2 = os.path.basename(edf_path_c2).split('.')[0]\n",
    "        epoch_c2 = process_file(edf_path_c2,csv_path_c2, filename_c2)\n",
    "\n",
    "        #----------------------------------------Session-2----------------------------------------\n",
    "\n",
    "        edf_path_c3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Data\\\\P{}-Ss2-X1-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_c3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Mark\\\\P{}-Ss2-X1-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c3 = os.path.basename(edf_path_c3).split('.')[0]\n",
    "        epoch_c3 = process_file(edf_path_c3,csv_path_c3, filename_c3)\n",
    "\n",
    "        edf_path_c4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Data\\\\P{}-Ss2-X2-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_c4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Mark\\\\P{}-Ss2-X2-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c4 = os.path.basename(edf_path_c4).split('.')[0]\n",
    "        epoch_c4 = process_file(edf_path_c4,csv_path_c4, filename_c4)\n",
    "\n",
    "        #----------------------------------------Session-3----------------------------------------\n",
    "\n",
    "        edf_path_c5 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Data\\\\P{}-Ss3-X1-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_c5 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Mark\\\\P{}-Ss3-X1-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c5 = os.path.basename(edf_path_c5).split('.')[0]\n",
    "        epoch_c5 = process_file(edf_path_c5,csv_path_c5, filename_c5)\n",
    "\n",
    "        edf_path_c6 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Data\\\\P{}-Ss3-X2-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_c6 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Mark\\\\P{}-Ss3-X2-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c6 = os.path.basename(edf_path_c6).split('.')[0]\n",
    "        epoch_c6 = process_file(edf_path_c6,csv_path_c6, filename_c6)\n",
    "\n",
    "        #----------------------------------------Session-4----------------------------------------\n",
    "\n",
    "        edf_path_c7 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Data\\\\P{}-Ss4-X1-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_c7 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Mark\\\\P{}-Ss4-X1-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c7 = os.path.basename(edf_path_c7).split('.')[0]\n",
    "        epoch_c7 = process_file(edf_path_c7,csv_path_c7, filename_c7)\n",
    "\n",
    "        edf_path_c8 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Data\\\\P{}-Ss4-X2-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_c8 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Mark\\\\P{}-Ss4-X2-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_c8 = os.path.basename(edf_path_c8).split('.')[0]\n",
    "        epoch_c8 = process_file(edf_path_c8,csv_path_c8, filename_c8)\n",
    " \n",
    "        # Batch 1 training purposes\n",
    "        Epoch_C = mne.concatenate_epochs([epoch_c1, epoch_c2, epoch_c3, epoch_c4, epoch_c5, epoch_c6, epoch_c7, epoch_c8])\n",
    "\n",
    "\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "        # RSVP with Body Movement ( X4 )\n",
    "        # Body ---> b\n",
    "\n",
    "        #----------------------------------------Session-1----------------------------------------\n",
    "\n",
    "        # Raw EEG edf file path\n",
    "        edf_path_b1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Data\\\\P{}-Ss1-X4-eeg.edf'.format(P_Num, P_Num)\n",
    "\n",
    "        # RSVP image labels file path\n",
    "        csv_path_b1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Mark\\\\P{}-Ss1-X4-mrk.csv'.format(P_Num, P_Num)\n",
    "\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_b1 = os.path.basename(edf_path_b1).split('.')[0]\n",
    "        epoch_b1 = process_file(edf_path_b1,csv_path_b1, filename_b1)\n",
    "\n",
    "\n",
    "        #----------------------------------------Session-2----------------------------------------\n",
    "\n",
    "        edf_path_b2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Data\\\\P{}-Ss2-X4-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_b2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Mark\\\\P{}-Ss2-X4-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_b2 = os.path.basename(edf_path_b2).split('.')[0]\n",
    "        epoch_b2 = process_file(edf_path_b2,csv_path_b2, filename_b2)\n",
    "\n",
    "        #----------------------------------------Session-3----------------------------------------\n",
    "\n",
    "        edf_path_b3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Data\\\\P{}-Ss3-X4-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_b3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Mark\\\\P{}-Ss3-X4-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_b3 = os.path.basename(edf_path_b3).split('.')[0]\n",
    "        epoch_b3 = process_file(edf_path_b3,csv_path_b3, filename_b3)\n",
    "\n",
    "        #----------------------------------------Session-4----------------------------------------\n",
    "\n",
    "        edf_path_b4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Data\\\\P{}-Ss4-X4-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_b4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Mark\\\\P{}-Ss4-X4-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_b4 = os.path.basename(edf_path_b4).split('.')[0]\n",
    "        epoch_b4 = process_file(edf_path_b4,csv_path_b4, filename_b4)\n",
    "\n",
    "        #------------------\n",
    "        Epoch_B = mne.concatenate_epochs([epoch_b1, epoch_b2, epoch_b3, epoch_b4])\n",
    "\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "        # RSVP with Talking ( X6 )\n",
    "        # Talking ---> t\n",
    "\n",
    "        #----------------------------------------Session-1----------------------------------------\n",
    "\n",
    "        # Raw EEG edf file path\n",
    "        edf_path_t1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Data\\\\P{}-Ss1-X6-eeg.edf'.format(P_Num, P_Num)\n",
    "\n",
    "        # RSVP image labels file path\n",
    "        csv_path_t1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Mark\\\\P{}-Ss1-X6-mrk.csv'.format(P_Num, P_Num)\n",
    "\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_t1 = os.path.basename(edf_path_t1).split('.')[0]\n",
    "        epoch_t1 = process_file(edf_path_t1,csv_path_t1, filename_t1)\n",
    "\n",
    "\n",
    "        #----------------------------------------Session-2----------------------------------------\n",
    "\n",
    "        edf_path_t2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Data\\\\P{}-Ss2-X6-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_t2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Mark\\\\P{}-Ss2-X6-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_t2 = os.path.basename(edf_path_t2).split('.')[0]\n",
    "        epoch_t2 = process_file(edf_path_t2,csv_path_t2, filename_t2)\n",
    "\n",
    "        #----------------------------------------Session-3----------------------------------------\n",
    "\n",
    "        edf_path_t3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Data\\\\P{}-Ss3-X6-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_t3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Mark\\\\P{}-Ss3-X6-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_t3 = os.path.basename(edf_path_t3).split('.')[0]\n",
    "        epoch_t3 = process_file(edf_path_t3,csv_path_t3, filename_t3)\n",
    "\n",
    "        #----------------------------------------Session-4----------------------------------------\n",
    "\n",
    "        edf_path_t4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Data\\\\P{}-Ss4-X6-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_t4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Mark\\\\P{}-Ss4-X6-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_t4 = os.path.basename(edf_path_t4).split('.')[0]\n",
    "        epoch_t4 = process_file(edf_path_t4,csv_path_t4, filename_t4)\n",
    "\n",
    "        #------------------\n",
    "        Epoch_T = mne.concatenate_epochs([epoch_t1, epoch_t2, epoch_t3 ,epoch_t4])\n",
    "\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        # RSVP with Head Movement ( X8 )\n",
    "        # Head ---> h\n",
    "\n",
    "        #----------------------------------------Session-1----------------------------------------\n",
    "\n",
    "        # Raw EEG edf file path\n",
    "        edf_path_h1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Data\\\\P{}-Ss1-X8-eeg.edf'.format(P_Num, P_Num)\n",
    "\n",
    "        # RSVP image labels file path\n",
    "        csv_path_h1 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-1\\\\Mark\\\\P{}-Ss1-X8-mrk.csv'.format(P_Num, P_Num)\n",
    "\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_h1 = os.path.basename(edf_path_h1).split('.')[0]\n",
    "        epoch_h1 = process_file(edf_path_h1,csv_path_h1, filename_h1)\n",
    "\n",
    "\n",
    "        #----------------------------------------Session-2----------------------------------------\n",
    "\n",
    "        edf_path_h2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Data\\\\P{}-Ss2-X8-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_h2 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-2\\\\Mark\\\\P{}-Ss2-X8-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_h2 = os.path.basename(edf_path_h2).split('.')[0]\n",
    "        epoch_h2 = process_file(edf_path_h2,csv_path_h2, filename_h2)\n",
    "\n",
    "        #----------------------------------------Session-3----------------------------------------\n",
    "\n",
    "        edf_path_h3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Data\\\\P{}-Ss3-X8-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_h3 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-3\\\\Mark\\\\P{}-Ss3-X8-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_h3 = os.path.basename(edf_path_h3).split('.')[0]\n",
    "        epoch_h3 = process_file(edf_path_h3,csv_path_h3, filename_h3)\n",
    "\n",
    "        #----------------------------------------Session-4----------------------------------------\n",
    "\n",
    "        edf_path_h4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Data\\\\P{}-Ss4-X8-eeg.edf'.format(P_Num, P_Num)\n",
    "        csv_path_h4 = r'C:\\\\Users\\\\Data A\\\\P{}\\\\Session-4\\\\Mark\\\\P{}-Ss4-X8-mrk.csv'.format(P_Num, P_Num)\n",
    "        #Processing the files to extract Epochs using the defined function 'process_file'\n",
    "        filename_h4 = os.path.basename(edf_path_h4).split('.')[0]\n",
    "        epoch_h4 = process_file(edf_path_h4,csv_path_h4, filename_h4)\n",
    "\n",
    "        #------------------\n",
    "        Epoch_H = mne.concatenate_epochs([epoch_h1,epoch_h2, epoch_h3, epoch_h4])\n",
    "        \n",
    "        \n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------       \n",
    "        #THR=100e-6 #Threshold\n",
    "        # Set your peak-to-peak threshold\n",
    "        p2p_threshold_C = THR  # #you can change it.. may be 100e-6 as well\n",
    "        p2p_threshold_B = THR  # #you can change it.. may be 100e-6 as well\n",
    "        p2p_threshold_T = THR  # #you can change it.. may be 100e-6 as well\n",
    "        p2p_threshold_H = THR  # #you can change it.. may be 100e-6 as well\n",
    "\n",
    "        # Make the copy of epochs\n",
    "        Epoch_C_All=Epoch_C.copy()\n",
    "        Epoch_B_All=Epoch_B.copy()\n",
    "        Epoch_T_All=Epoch_T.copy()\n",
    "        Epoch_H_All=Epoch_H.copy()\n",
    "        \n",
    "        # Get the data from epochs\n",
    "        Epoch_C_All_Data = Epoch_C_All.get_data()\n",
    "        Epoch_B_All_Data = Epoch_B_All.get_data()\n",
    "        Epoch_T_All_Data = Epoch_T_All.get_data()\n",
    "        Epoch_H_All_Data = Epoch_H_All.get_data()\n",
    "    \n",
    "\n",
    "        # Calculate the peak-to-peak amplitude along the time axis\n",
    "        p2p_amplitude_C = np.ptp(Epoch_C_All_Data, axis=2)\n",
    "        p2p_amplitude_B = np.ptp(Epoch_B_All_Data, axis=2)\n",
    "        p2p_amplitude_T = np.ptp(Epoch_T_All_Data, axis=2)\n",
    "        p2p_amplitude_H = np.ptp(Epoch_H_All_Data, axis=2)\n",
    "        \n",
    "\n",
    "        # Find epochs that exceed the peak-to-peak threshold\n",
    "        bad_epochs_C = np.where(p2p_amplitude_C > p2p_threshold_C)[0]\n",
    "        bad_epochs_B = np.where(p2p_amplitude_B > p2p_threshold_B)[0]\n",
    "        bad_epochs_T = np.where(p2p_amplitude_T > p2p_threshold_T)[0]\n",
    "        bad_epochs_H = np.where(p2p_amplitude_H > p2p_threshold_H)[0]\n",
    "        \n",
    "\n",
    "        # Clean Epochs :- Drop bad epochs from the Epochs object\n",
    "        Clean_epochs_C = Epoch_C_All.copy()\n",
    "        Clean_epochs_C.drop(bad_epochs_C)\n",
    "        \n",
    "        Clean_epochs_B = Epoch_B_All.copy()\n",
    "        Clean_epochs_B.drop(bad_epochs_B)\n",
    "        \n",
    "        Clean_epochs_T = Epoch_T_All.copy()\n",
    "        Clean_epochs_T.drop(bad_epochs_T)\n",
    "        \n",
    "        Clean_epochs_H = Epoch_H_All.copy()\n",
    "        Clean_epochs_H.drop(bad_epochs_H)\n",
    "\n",
    "        # Rejected Epochs \n",
    "        \n",
    "        Clean_indices_C = set(Clean_epochs_C.selection)                # Find the indices of clean epochs\n",
    "        All_indices_C = set(Epoch_C_All.selection)                     # Find the indices of all epochs\n",
    "        Rejected_indices_C = All_indices_C - Clean_indices_C           # Find the indices of rejected epochs\n",
    "        Rejected_indices_C = list(Rejected_indices_C)                  # making a list of indices\n",
    "        Rejected_indices_C = np.array(Rejected_indices_C).astype(int)  # Converting the list of indices to int\n",
    "        Rejected_epochs_C = Epoch_C_All[Rejected_indices_C]\n",
    "        \n",
    "        Clean_indices_B = set(Clean_epochs_B.selection)                # Find the indices of clean epochs\n",
    "        All_indices_B = set(Epoch_B_All.selection)                     # Find the indices of all epochs\n",
    "        Rejected_indices_B = All_indices_B - Clean_indices_B           # Find the indices of rejected epochs\n",
    "        Rejected_indices_B = list(Rejected_indices_B)                  # making a list of indices\n",
    "        Rejected_indices_B = np.array(Rejected_indices_B).astype(int)  # Converting the list of indices to int\n",
    "        Rejected_epochs_B = Epoch_B_All[Rejected_indices_B] \n",
    "        \n",
    "        Clean_indices_T = set(Clean_epochs_T.selection)                # Find the indices of clean epochs\n",
    "        All_indices_T = set(Epoch_T_All.selection)                     # Find the indices of all epochs\n",
    "        Rejected_indices_T = All_indices_T - Clean_indices_T           # Find the indices of rejected epochs\n",
    "        Rejected_indices_T = list(Rejected_indices_T)                  # making a list of indices\n",
    "        Rejected_indices_T = np.array(Rejected_indices_T).astype(int)  # Converting the list of indices to int\n",
    "        Rejected_epochs_T = Epoch_T_All[Rejected_indices_T]\n",
    "        \n",
    "        Clean_indices_H = set(Clean_epochs_H.selection)                # Find the indices of clean epochs\n",
    "        All_indices_H = set(Epoch_H_All.selection)                     # Find the indices of all epochs\n",
    "        Rejected_indices_H = All_indices_H - Clean_indices_H           # Find the indices of rejected epochs\n",
    "        Rejected_indices_H = list(Rejected_indices_H)                  # making a list of indices\n",
    "        Rejected_indices_H = np.array(Rejected_indices_H).astype(int)  # Converting the list of indices to int\n",
    "        Rejected_epochs_H = Epoch_H_All[Rejected_indices_H]\n",
    "        \n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------    \n",
    "        \n",
    "        # Getting the percentage of dropped/bad epochs\n",
    "\n",
    "        percentage_dropped_C=(len(Rejected_epochs_C) / len(Epoch_C_All)) * 100\n",
    "        \n",
    "        percentage_dropped_B=(len(Rejected_epochs_B) / len(Epoch_B_All)) * 100\n",
    "        \n",
    "        percentage_dropped_T=(len(Rejected_epochs_T) / len(Epoch_T_All)) * 100   \n",
    "        \n",
    "        percentage_dropped_H=(len(Rejected_epochs_H) / len(Epoch_H_All)) * 100\n",
    "        \n",
    "\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##----------------------------------------Part 1--------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ## Train on Traditional RSVP (Clean) and Test on Traditional RSVP (Clean)\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "        Event_Clean_epochs_C = Clean_epochs_C.events\n",
    "        standard_indices = np.where(Event_Clean_epochs_C[:, -1] == 2)[0]\n",
    "        target_indices = np.where(Event_Clean_epochs_C[:, -1] == 1)[0]\n",
    "        \n",
    "        # Combine indices for both classes\n",
    "        all_indices_Clean_epochs_C = np.concatenate([standard_indices, target_indices])\n",
    "        \n",
    "        # Fix the seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Randomly shuffle the combined indices\n",
    "        np.random.shuffle(all_indices_Clean_epochs_C)\n",
    "        \n",
    "        # Select 1080 events for training\n",
    "        num_training_events = 1080\n",
    "        training_indices_Clean_epochs_C = all_indices_Clean_epochs_C[:num_training_events]\n",
    "        \n",
    "        # The remaining events will be used for testing\n",
    "        testing_indices_Clean_epochs_C = all_indices_Clean_epochs_C[num_training_events:]\n",
    "        \n",
    "        # Separate Epoch_A into training and testing sets\n",
    "        Clean_epochs_C_training = Clean_epochs_C[training_indices_Clean_epochs_C]\n",
    "        \n",
    "        Clean_epochs_C_testing = Clean_epochs_C[testing_indices_Clean_epochs_C]\n",
    "        \n",
    "        \n",
    "        # Traditional RSVP Clean Epochs for Training\n",
    "        Data_Clean_C_Training = Clean_epochs_C_training.get_data()\n",
    "        Data_Clean_C_Training = Data_Clean_C_Training.reshape(Data_Clean_C_Training.shape[0], -1)\n",
    "        Events_Clean_C_Training = Clean_epochs_C_training.events[:, -1]\n",
    "        \n",
    "        # Traditional RSVP Clean Epochs for Testing\n",
    "        Data_Clean_C_Testing = Clean_epochs_C_testing.get_data()\n",
    "        Data_Clean_C_Testing = Data_Clean_C_Testing.reshape(Data_Clean_C_Testing.shape[0], -1)\n",
    "        Events_Clean_C_Testing = Clean_epochs_C_testing.events[:, -1]\n",
    "        \n",
    "        ## BayesianRidge :- Traditional RSVP\n",
    "        \n",
    "        scaler = StandardScaler() # Normalize the features\n",
    "        \n",
    "        # Create BayesianRidge model\n",
    "        mod = BayesianRidge()\n",
    "        \n",
    "        # Create a pipeline with normalization and BayesianRidge\n",
    "        clf1 = make_pipeline(scaler, mod)\n",
    "        \n",
    "        # Fit the model\n",
    "        clf1.fit(Data_Clean_C_Training, Events_Clean_C_Training)\n",
    "        \n",
    "        y_pred_clean = clf1.predict(Data_Clean_C_Testing)\n",
    "        rocauc_clean= roc_auc_score(Events_Clean_C_Testing, y_pred_clean)\n",
    "        print(P_Num, \" -Model Clean- \", round(rocauc_clean, 2))\n",
    "\n",
    "\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##----------------------------------------Part 2--------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ## Train on Traditional RSVP (Clean+Noise) and Test on Traditional RSVP (Clean+Noise)\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "        Event_all_epochs_C=Epoch_C_All.events\n",
    "        \n",
    "        standard_indices = np.where(Event_all_epochs_C[:, -1] == 2)[0]\n",
    "        len(standard_indices)\n",
    "        \n",
    "        target_indices = np.where(Event_all_epochs_C[:, -1] == 1)[0]\n",
    "        len(target_indices)\n",
    "        \n",
    "        # Combine indices for both classes\n",
    "        all_indices_all_epochs_C = np.concatenate([standard_indices, target_indices])\n",
    "        \n",
    "        # Fix the seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Randomly shuffle the combined indices\n",
    "        np.random.shuffle(all_indices_all_epochs_C)\n",
    "        \n",
    "        # Select 1080 events for training\n",
    "        num_training_events = 1080\n",
    "        training_indices_all_epochs_C = all_indices_all_epochs_C[:num_training_events]\n",
    "        \n",
    "        # The remaining events will be used for testing\n",
    "        testing_indices_all_epochs_C = all_indices_all_epochs_C[num_training_events:]\n",
    "        \n",
    "        # Separate Epoch_A into training and testing sets\n",
    "        All_epochs_C_training = Epoch_C_All[training_indices_all_epochs_C]\n",
    "        \n",
    "        All_epochs_C_testing = Epoch_C_All[testing_indices_all_epochs_C]\n",
    "    \n",
    "            \n",
    "        # Traditional RSVP Clean + Noise Epochs for Training\n",
    "        Data_All_C_Training = All_epochs_C_training.get_data()\n",
    "        Data_All_C_Training = Data_All_C_Training.reshape(Data_All_C_Training.shape[0], -1)\n",
    "        Events_All_C_Training = All_epochs_C_training.events[:, -1]\n",
    "        \n",
    "        # Traditional RSVP Clean + Noise Epochs for Testing\n",
    "        Data_All_C_Testing = All_epochs_C_testing.get_data()\n",
    "        Data_All_C_Testing = Data_All_C_Testing.reshape(Data_All_C_Testing.shape[0], -1)\n",
    "        Events_All_C_Testing = All_epochs_C_testing.events[:, -1]\n",
    "    \n",
    "        ## BayesianRidge :- Traditional RSVP\n",
    "        \n",
    "        scaler = StandardScaler() # Normalize the features\n",
    "        \n",
    "        # Create BayesianRidge model\n",
    "        mod = BayesianRidge()\n",
    "        \n",
    "        # Create a pipeline with normalization and BayesianRidge\n",
    "        clf2 = make_pipeline(scaler, mod)\n",
    "        \n",
    "        # Fit the model\n",
    "        clf2.fit(Data_All_C_Training, Events_All_C_Training)\n",
    "        \n",
    "        y_pred_all = clf2.predict(Data_All_C_Testing)\n",
    "        rocauc_all= roc_auc_score(Events_All_C_Testing, y_pred_all)\n",
    "        print(P_Num, \" -Model Clean+Noise- \", round(rocauc_all, 2))\n",
    "\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##----------------------------------------Part 3--------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ## Train on 3 Categories (Body+Talk+Head) Combined RSVP (Clean) and Test on 3 Categories (Body+Talk+Head) Combined RSVP (Clean)\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "        if percentage_dropped_H < 90:\n",
    "            Epoch_3Cat = mne.concatenate_epochs([Clean_epochs_H, Clean_epochs_B, Clean_epochs_T])\n",
    "            # Make the copy of epochs\n",
    "            Epoch_3Cat_Clean=Epoch_3Cat.copy()\n",
    "               \n",
    "            Event_clean_epochs_3Cat=Epoch_3Cat_Clean.events\n",
    "            \n",
    "            standard_indices = np.where(Event_clean_epochs_3Cat[:, -1] == 2)[0]\n",
    "            len(standard_indices)\n",
    "            \n",
    "            target_indices = np.where(Event_clean_epochs_3Cat[:, -1] == 1)[0]\n",
    "            len(target_indices)\n",
    "            \n",
    "            # Combine indices for both classes\n",
    "            all_indices_clean_epochs_3Cat = np.concatenate([standard_indices, target_indices])\n",
    "            \n",
    "            # Fix the seed for reproducibility\n",
    "            np.random.seed(42)\n",
    "            \n",
    "            # Randomly shuffle the combined indices\n",
    "            np.random.shuffle(all_indices_clean_epochs_3Cat)\n",
    "            \n",
    "            # Select 1080 events for training\n",
    "            num_training_events = 1080\n",
    "            training_indices_clean_epochs_3Cat = all_indices_clean_epochs_3Cat[:num_training_events]\n",
    "            \n",
    "            # The remaining events will be used for testing\n",
    "            testing_indices_clean_epochs_3Cat = all_indices_clean_epochs_3Cat[num_training_events:]\n",
    "            \n",
    "            # Separate Epoch_A into training and testing sets\n",
    "            Clean_epochs_3Cat_training = Epoch_3Cat_Clean[training_indices_clean_epochs_3Cat]\n",
    "            \n",
    "            Clean_epochs_3Cat_testing = Epoch_3Cat_Clean[testing_indices_clean_epochs_3Cat]\n",
    "        \n",
    "            if len(Epoch_3Cat_Clean) < 1080:\n",
    "                # Traditional RSVP Clean + Noise Epochs for Training\n",
    "                Data_Clean_3Cat_Training = 0\n",
    "                Data_Clean_3Cat_Training = 0\n",
    "                Events_Clean_3Cat_Training = 0\n",
    "                \n",
    "                # Traditional RSVP Clean + Noise Epochs for Testing\n",
    "                Data_Clean_3Cat_Testing = 0\n",
    "                Data_Clean_3Cat_Testing = 0\n",
    "                Events_Clean_3Cat_Testing = 0\n",
    "    \n",
    "                rocauc_3Cat_clean=0\n",
    "                print(\"Cannot Classify: total number of clean trials are less than 1080\")\n",
    "    \n",
    "            else:\n",
    "                # Traditional RSVP Clean + Noise Epochs for Training\n",
    "                Data_Clean_3Cat_Training = Clean_epochs_3Cat_training.get_data()\n",
    "                Data_Clean_3Cat_Training = Data_Clean_3Cat_Training.reshape(Data_Clean_3Cat_Training.shape[0], -1)\n",
    "                Events_Clean_3Cat_Training = Clean_epochs_3Cat_training.events[:, -1]\n",
    "                \n",
    "                # Traditional RSVP Clean + Noise Epochs for Testing\n",
    "                Data_Clean_3Cat_Testing = Clean_epochs_3Cat_testing.get_data()\n",
    "                Data_Clean_3Cat_Testing = Data_Clean_3Cat_Testing.reshape(Data_Clean_3Cat_Testing.shape[0], -1)\n",
    "                Events_Clean_3Cat_Testing = Clean_epochs_3Cat_testing.events[:, -1]\n",
    "        \n",
    "                ## BayesianRidge :- Traditional RSVP\n",
    "                \n",
    "                scaler = StandardScaler() # Normalize the features\n",
    "                \n",
    "                # Create BayesianRidge model\n",
    "                mod = BayesianRidge()\n",
    "                \n",
    "                # Create a pipeline with normalization and BayesianRidge\n",
    "                clf3 = make_pipeline(scaler, mod)\n",
    "                \n",
    "                # Fit the model\n",
    "                clf3.fit(Data_Clean_3Cat_Training, Events_Clean_3Cat_Training)\n",
    "                \n",
    "                y_pred_3Cat_clean = clf3.predict(Data_Clean_3Cat_Testing)\n",
    "                rocauc_3Cat_clean= roc_auc_score(Events_Clean_3Cat_Testing, y_pred_3Cat_clean)\n",
    "                print(P_Num, \" -Model_3Cat_Clean- \", round(rocauc_3Cat_clean, 2))\n",
    "\n",
    "        else:\n",
    "            rocauc_3Cat_clean=0\n",
    "            print(\"Cannot Classify: dropped percentage of head catergory is more than 90% \")\n",
    "\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##----------------------------------------Part 4--------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ## Train on 3 Categories (Body+Talk+Head) Combined RSVP (Clean+Noise) and Test on 3 Categories (Body+Talk+Head) Combined RSVP (Clean+Noise)\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "        Epoch_3Cat = mne.concatenate_epochs([Epoch_H, Epoch_B, Epoch_T])\n",
    "    \n",
    "        # Make the copy of epochs\n",
    "        Epoch_3Cat_All=Epoch_3Cat.copy()\n",
    "    \n",
    "        Event_all_epochs_3Cat=Epoch_3Cat_All.events\n",
    "        \n",
    "        standard_indices = np.where(Event_all_epochs_3Cat[:, -1] == 2)[0]\n",
    "        len(standard_indices)\n",
    "        \n",
    "        target_indices = np.where(Event_all_epochs_3Cat[:, -1] == 1)[0]\n",
    "        len(target_indices)\n",
    "        \n",
    "        # Combine indices for both classes\n",
    "        all_indices_all_epochs_3Cat = np.concatenate([standard_indices, target_indices])\n",
    "        \n",
    "        # Fix the seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Randomly shuffle the combined indices\n",
    "        np.random.shuffle(all_indices_all_epochs_3Cat)\n",
    "        \n",
    "        # Select 1080 events for training\n",
    "        num_training_events = 1080\n",
    "        training_indices_all_epochs_3Cat = all_indices_all_epochs_3Cat[:num_training_events]\n",
    "        \n",
    "        # The remaining events will be used for testing\n",
    "        testing_indices_all_epochs_3Cat = all_indices_all_epochs_3Cat[num_training_events:]\n",
    "        \n",
    "        # Separate Epoch_A into training and testing sets\n",
    "        All_epochs_3Cat_training = Epoch_3Cat_All[training_indices_all_epochs_3Cat]\n",
    "        \n",
    "        All_epochs_3Cat_testing = Epoch_3Cat_All[testing_indices_all_epochs_3Cat]\n",
    "    \n",
    "            \n",
    "        # Traditional RSVP Clean + Noise Epochs for Training\n",
    "        Data_All_3Cat_Training = All_epochs_3Cat_training.get_data()\n",
    "        Data_All_3Cat_Training = Data_All_3Cat_Training.reshape(Data_All_3Cat_Training.shape[0], -1)\n",
    "        Events_All_3Cat_Training = All_epochs_3Cat_training.events[:, -1]\n",
    "        \n",
    "        # Traditional RSVP Clean + Noise Epochs for Testing\n",
    "        Data_All_3Cat_Testing = All_epochs_3Cat_testing.get_data()\n",
    "        Data_All_3Cat_Testing = Data_All_3Cat_Testing.reshape(Data_All_3Cat_Testing.shape[0], -1)\n",
    "        Events_All_3Cat_Testing = All_epochs_3Cat_testing.events[:, -1]\n",
    "    \n",
    "        ## BayesianRidge :- Traditional RSVP\n",
    "        \n",
    "        scaler = StandardScaler() # Normalize the features\n",
    "        \n",
    "        # Create BayesianRidge model\n",
    "        mod = BayesianRidge()\n",
    "        \n",
    "        # Create a pipeline with normalization and BayesianRidge\n",
    "        clf4 = make_pipeline(scaler, mod)\n",
    "        \n",
    "        # Fit the model\n",
    "        clf4.fit(Data_All_3Cat_Training, Events_All_3Cat_Training)\n",
    "        \n",
    "        y_pred_3Cat_all = clf4.predict(Data_All_3Cat_Testing)\n",
    "        rocauc_3Cat_all= roc_auc_score(Events_All_3Cat_Testing, y_pred_3Cat_all)\n",
    "        print(P_Num, \" -Model_3Cat_Clean+Noise- \", round(rocauc_3Cat_all, 2))\n",
    "\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##----------------------------------------Part 5--------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ## Train on 3 Categories (Body+Talk+Head) Combined RSVP (Clean+Noise) and Test on Traditional RSVP (Clean)\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "        if percentage_dropped_H < 90:\n",
    "            Epoch_3Cat = mne.concatenate_epochs([Clean_epochs_H, Clean_epochs_B, Clean_epochs_T])\n",
    "            # Make the copy of epochs\n",
    "            Epoch_3Cat_Clean=Epoch_3Cat.copy()\n",
    "               \n",
    "            Event_clean_epochs_3Cat=Epoch_3Cat_Clean.events\n",
    "            \n",
    "            standard_indices = np.where(Event_clean_epochs_3Cat[:, -1] == 2)[0]\n",
    "            len(standard_indices)\n",
    "            \n",
    "            target_indices = np.where(Event_clean_epochs_3Cat[:, -1] == 1)[0]\n",
    "            len(target_indices)\n",
    "            \n",
    "            # Combine indices for both classes\n",
    "            all_indices_clean_epochs_3Cat = np.concatenate([standard_indices, target_indices])\n",
    "            \n",
    "            # Fix the seed for reproducibility\n",
    "            np.random.seed(42)\n",
    "            \n",
    "            # Randomly shuffle the combined indices\n",
    "            np.random.shuffle(all_indices_clean_epochs_3Cat)\n",
    "            \n",
    "            # Select 1080 events for training\n",
    "            num_training_events = 1080\n",
    "            training_indices_clean_epochs_3Cat = all_indices_clean_epochs_3Cat[:num_training_events] \n",
    "           \n",
    "            # Separate Epoch_A into training and testing sets\n",
    "            Clean_epochs_3Cat_training = Epoch_3Cat_Clean[training_indices_clean_epochs_3Cat]\n",
    "            \n",
    "        \n",
    "            if len(Epoch_3Cat_Clean) < 1080:\n",
    "                # Traditional RSVP Clean + Noise Epochs for Training\n",
    "                Data_Clean_3Cat_Training = 0\n",
    "                Data_Clean_3Cat_Training = 0\n",
    "                Events_Clean_3Cat_Training = 0\n",
    "                \n",
    "                # Traditional RSVP Clean + Noise Epochs for Testing\n",
    "                Data_Clean_3Cat_Testing = 0\n",
    "                Data_Clean_3Cat_Testing = 0\n",
    "                Events_Clean_3Cat_Testing = 0\n",
    "    \n",
    "                rocauc_3Cat_clean=0\n",
    "                print(\"Cannot Classify: total number of clean trials are less than 1080\")\n",
    "    \n",
    "            else:\n",
    "                # Traditional RSVP Clean + Noise Epochs for Training\n",
    "                Data_Clean_3Cat_Training = Clean_epochs_3Cat_training.get_data()\n",
    "                Data_Clean_3Cat_Training = Data_Clean_3Cat_Training.reshape(Data_Clean_3Cat_Training.shape[0], -1)\n",
    "                Events_Clean_3Cat_Training = Clean_epochs_3Cat_training.events[:, -1] \n",
    "                \n",
    "                # Traditional RSVP (Clean) Epochs for Testing\n",
    "                Data_Clean_C_Testing = Clean_epochs_C_testing.get_data()\n",
    "                Data_Clean_C_Testing = Data_Clean_C_Testing.reshape(Data_Clean_C_Testing.shape[0], -1)\n",
    "                Events_Clean_C_Testing = Clean_epochs_C_testing.events[:, -1]\n",
    "            \n",
    "                ## BayesianRidge :- Traditional RSVP\n",
    "                \n",
    "                scaler = StandardScaler() # Normalize the features\n",
    "                \n",
    "                # Create BayesianRidge model\n",
    "                mod = BayesianRidge()\n",
    "                \n",
    "                # Create a pipeline with normalization and BayesianRidge\n",
    "                clf5 = make_pipeline(scaler, mod)\n",
    "                \n",
    "                # Fit the model\n",
    "                clf5.fit(Data_Clean_3Cat_Training, Events_Clean_3Cat_Training)\n",
    "                \n",
    "                y_pred_Tr3Cat_TsC = clf5.predict(Data_Clean_C_Testing)\n",
    "                rocauc_Tr3Cat_TsC= roc_auc_score(Events_Clean_C_Testing, y_pred_Tr3Cat_TsC)\n",
    "                print(P_Num, \" -Model_Tr3Cat_Clean_TsClean- \", round(rocauc_Tr3Cat_TsC, 2))\n",
    "\n",
    "        else:\n",
    "            rocauc_Tr3Cat_TsC=0\n",
    "            print(\"Cannot Classify: dropped percentage of head catergory is more than 90% \")\n",
    "        \n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##----------------------------------------Part 6--------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ## Train on 3 Categories (Body+Talk+Head) Combined RSVP (Clean+Noise) and Test on Traditional RSVP (Clean+Noise)\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "        Epoch_3Cat = mne.concatenate_epochs([Epoch_H, Epoch_B, Epoch_T])\n",
    "    \n",
    "        # Make the copy of epochs\n",
    "        Epoch_3Cat_All=Epoch_3Cat.copy()\n",
    "    \n",
    "        Event_all_epochs_3Cat=Epoch_3Cat_All.events\n",
    "        \n",
    "        standard_indices = np.where(Event_all_epochs_3Cat[:, -1] == 2)[0]\n",
    "        len(standard_indices)\n",
    "        \n",
    "        target_indices = np.where(Event_all_epochs_3Cat[:, -1] == 1)[0]\n",
    "        len(target_indices)\n",
    "        \n",
    "        # Combine indices for both classes\n",
    "        all_indices_all_epochs_3Cat = np.concatenate([standard_indices, target_indices])\n",
    "        \n",
    "        # Fix the seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Randomly shuffle the combined indices\n",
    "        np.random.shuffle(all_indices_all_epochs_3Cat)\n",
    "        \n",
    "        # Select 1080 events for training\n",
    "        num_training_events = 1080\n",
    "        training_indices_all_epochs_3Cat = all_indices_all_epochs_3Cat[:num_training_events]\n",
    "        \n",
    "        \n",
    "        # Separate Epoch_A into training and testing sets\n",
    "        All_epochs_3Cat_training = Epoch_3Cat_All[training_indices_all_epochs_3Cat]\n",
    "        \n",
    "        \n",
    "            \n",
    "        # Artifactual RSVP (Clean + Noise) Epochs for Training\n",
    "        Data_All_3Cat_Training = All_epochs_3Cat_training.get_data()\n",
    "        Data_All_3Cat_Training = Data_All_3Cat_Training.reshape(Data_All_3Cat_Training.shape[0], -1)\n",
    "        Events_All_3Cat_Training = All_epochs_3Cat_training.events[:, -1]\n",
    "        \n",
    "        # Traditional RSVP (Clean+Bad) Epochs for Testing\n",
    "        Data_All_C_Testing = All_epochs_C_testing.get_data()\n",
    "        Data_All_C_Testing = Data_All_C_Testing.reshape(Data_All_C_Testing.shape[0], -1)\n",
    "        Events_All_C_Testing = All_epochs_C_testing.events[:, -1]\n",
    "    \n",
    "        ## BayesianRidge :- Traditional RSVP\n",
    "        \n",
    "        scaler = StandardScaler() # Normalize the features\n",
    "        \n",
    "        # Create BayesianRidge model\n",
    "        mod = BayesianRidge()\n",
    "        \n",
    "        # Create a pipeline with normalization and BayesianRidge\n",
    "        clf6 = make_pipeline(scaler, mod)\n",
    "        \n",
    "        # Fit the model\n",
    "        clf6.fit(Data_All_3Cat_Training, Events_All_3Cat_Training)\n",
    "        \n",
    "        y_pred_Tr3Cat_TsCB = clf6.predict(Data_All_C_Testing)\n",
    "        rocauc_Tr3Cat_TsCB= roc_auc_score(Events_All_C_Testing, y_pred_Tr3Cat_TsCB)\n",
    "        print(P_Num, \" -Model_Tr3Cat_Clean+Noise_TsCleanBad- \", round(rocauc_Tr3Cat_TsCB, 2))\n",
    "\n",
    "        \n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "        ##------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        X=00000000000.0 #  Just to add extrac column in csv file\n",
    "        \n",
    "        # Write the values to the CSV file\n",
    "        writer.writerow([P_Num, round(rocauc_clean, 2), X, round(rocauc_all, 2), X, round(rocauc_3Cat_clean, 2), X, round(rocauc_3Cat_all, 2), X, round(rocauc_Tr3Cat_TsC, 2), X, round(rocauc_Tr3Cat_TsCB, 2)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
